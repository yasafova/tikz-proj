# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZVjQPnQorC4gxBLefn4HQa7IWd87IltB
"""

!pip install -q datasets transformers peft accelerate bitsandbytes torchmetrics

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from PIL import Image
from datasets import load_dataset
from tqdm import tqdm
from transformers import T5TokenizerFast
from transformers import CLIPProcessor, CLIPModel, T5Tokenizer, T5ForConditionalGeneration
from peft import get_peft_model, LoraConfig, TaskType
from torchmetrics.multimodal import CLIPScore as TorchCLIPScore

from transformers import AutoTokenizer, T5ForConditionalGeneration

# Load models
clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
"""
tokenizer = T5TokenizerFast.from_pretrained("t5-base")
latex_tokens = [
    "\\documentclass", "\\usepackage", "\\begin", "\\end",
    "\\tikzstyle", "\\node", "\\draw", "\\fill", "\\path",
    "\\pgfplotsset", "\\setlength", "\\foreach", "\\addplot",
    "tikztonodes", "tikztarget", "\\fill", "\\node",
    "\\begin{tikzpicture}", "\\end{tikzpicture}",
    "{", "}", "[", "]", "(", ")", "%", ";", "\\n"
]
tokenizer.add_tokens(latex_tokens)
"""


tokenizer = AutoTokenizer.from_pretrained("Salesforce/codet5-base", trust_remote_code=True)
t5_model = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5-base")
t5_model.config.decoder_start_token_id = tokenizer.pad_token_id


#t5_model = T5ForConditionalGeneration.from_pretrained("t5-base")
#t5_model.resize_token_embeddings(len(tokenizer))

# Apply LoRA to T5 decoder
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q", "v"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM
)
t5_model = get_peft_model(t5_model, lora_config)

class TikZGenModel(nn.Module):
    def __init__(self, clip_model, t5_model):
        super().__init__()
        self.clip_model = clip_model
        self.t5_model = t5_model
        self.fusion = nn.Linear(clip_model.config.projection_dim + t5_model.config.d_model, t5_model.config.d_model)

    def forward(self, image, input_ids, attention_mask, labels=None):
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(pixel_values=image)

        text_features = self.t5_model.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]
        combined_features = torch.cat([text_features, image_features], dim=1)
        fused = self.fusion(combined_features).unsqueeze(1)

        return self.t5_model(inputs_embeds=fused, labels=labels)

# Load full dataset (or first 15K examples)
full_data = load_dataset("nllg/datikz-v3", split="train[:5500]").shuffle(seed=42)

# Split 90% train / 10% test
split = full_data.train_test_split(test_size=0.1, seed=42)
train_raw = split["train"]
test_raw = split["test"]

preprocessed_train = []
preprocessed_test = []

def preprocess_dataset(dataset_split):
    processed = []
    for example in dataset_split:
        image = example['image'].convert("RGB")
        image_tensor = clip_processor(images=image, return_tensors="pt")['pixel_values'].squeeze(0)

        inputs = tokenizer(example['caption'], return_tensors="pt", padding="max_length", truncation=True, max_length=256)
        latex_code = example['code']
        # Optionally normalize if any weird characters
        if isinstance(latex_code, str):
            latex_code = latex_code.replace("\r\n", "\n").replace("\r", "\n")  # clean CRLF

        targets = tokenizer(
            latex_code,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=256  # increase to avoid cutting code
        )
        """
        print("Raw TikZ Code:", example['code'])
        decoded_tokens = tokenizer.convert_ids_to_tokens(targets['input_ids'].squeeze(0))
        print("Tokenized TikZ (tokens):", decoded_tokens)
        """
        processed.append({
            'image': image_tensor,
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'labels': targets['input_ids'].squeeze(0),
            'caption': example['caption'],
            'reference': example['code']
        })
    return processed

preprocessed_train = preprocess_dataset(train_raw)
preprocessed_test = preprocess_dataset(test_raw)

from torch.utils.data import Dataset

class TikzDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __getitem__(self, idx):
        return self.data[idx]

    def __len__(self):
        return len(self.data)

dataset_torch = TikzDataset(preprocessed_train)

def collate_fn(batch):
    return {
        'image': torch.stack([x['image'] for x in batch]),
        'input_ids': torch.stack([x['input_ids'] for x in batch]),
        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),
        'labels': torch.stack([x['labels'] for x in batch]),
        'caption': [x['caption'] for x in batch],
        'reference': [x['reference'] for x in batch],
    }

loader = DataLoader(dataset_torch, batch_size=4, shuffle=True, collate_fn=collate_fn)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TikZGenModel(clip_model, t5_model).to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
loader = DataLoader(dataset_torch, batch_size=4, shuffle=True, collate_fn=collate_fn)

model.train()
for epoch in range(8):
    loop = tqdm(loader, desc=f"Epoch {epoch+1}")
    for batch in loop:
        image = batch['image'].to(device)
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(image, input_ids, attention_mask, labels)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        loop.set_postfix(loss=loss.item())

    torch.save(model.state_dict(), f"/content/tikzgen_epoch{epoch+1}.pt")

"""
model = TikZGenModel(clip_model, t5_model).to(device)
#model.t5_model.config.decoder_start_token_id = tokenizer.pad_token_id
model.load_state_dict(torch.load("/content/tikzgen_epoch3.pt"))
"""

def generate_tikz(caption, image_tensor, device="cuda"):
    model.eval()
    model.to(device)

    start_id = tokenizer.eos_token_id if tokenizer.eos_token_id != 0 else tokenizer.pad_token_id
    model.t5_model.config.decoder_start_token_id = tokenizer.pad_token_id  # which is 0



    image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension
    inputs = tokenizer(caption, return_tensors="pt", padding=True, truncation=True, max_length=64)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    with torch.no_grad():
        text_features = model.t5_model.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        seq_len = text_features.size(1)
        image_features = model.clip_model.get_image_features(pixel_values=image_tensor).unsqueeze(1)
        image_features = image_features.expand(-1, seq_len, -1)
        fused = model.fusion(torch.cat([text_features, image_features], dim=-1))

        output_ids = model.t5_model.generate(
            #inputs_embeds=fused,
            input_ids=input_ids,
            max_length=256,
            num_beams=4,
            early_stopping=True,
            decoder_start_token_id=model.t5_model.config.decoder_start_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

test_data = preprocessed_test
#test_data = test_raw.select(range(20))
# Collect predictions and inputs
predictions, references, images, captions = [], [], [], []

for ex in test_data:
    pred = generate_tikz(ex['caption'], ex['image'])
    predictions.append(pred)
    references.append(ex['reference'])  # or ex['reference'] if using preprocessed_data
    images.append(ex['image'])
    captions.append(ex['caption'])

"""import pandas as pd

# Create a DataFrame from the lists
df = pd.DataFrame({
    "caption": captions,
    "prediction": predictions,
    "reference": references,
    "image_shape": [img.shape if hasattr(img, 'shape') else "N/A" for img in images]
})

# Save to CSV
csv_path = "/content/tikz_predictions.csv"
df.to_csv(csv_path, index=False)
print(f"CSV saved at: {csv_path}")

from google.colab import files
files.download("/content/tikz_predictions.csv")

for i in range(13):
    print("\nPrediction:")
    print(predictions[i])
    print("------------------------------")
print("\nCode 1 :")
print(references[9])
"""

from collections import Counter
from nltk.util import ngrams
import numpy as np

def crystal_bleu(candidate_list, references_list, n=4):
    """
    Simplified CrystalBLEU for evaluating text generation (e.g., TikZ).
    Based on n-gram overlap, with smoothing.
    """
    def count_ngrams(sequence, n):
        return Counter(ngrams(sequence, n)) if len(sequence) >= n else Counter()

    scores = []
    for candidate, references in zip(candidate_list, references_list):
        candidate_tokens = candidate.split()
        reference_tokens = references[0].split()

        precision_scores = []
        for i in range(1, n+1):
            cand_ng = count_ngrams(candidate_tokens, i)
            ref_ng = count_ngrams(reference_tokens, i)

            overlap = sum((cand_ng & ref_ng).values())
            total = max(sum(cand_ng.values()), 1)  # avoid division by zero
            precision_scores.append(overlap / total)

        score = np.exp(np.mean([np.log(p + 1e-8) for p in precision_scores]))  # geometric mean
        scores.append(score)

    return float(np.mean(scores))

cb = crystal_bleu(predictions, [[ref] for ref in references])
print("CrystalBLEU:", cb)

!pip install editdistance

import editdistance

def extended_edit_distance(preds, refs):
    total = 0
    for pred, ref in zip(preds, refs):
        dist = editdistance.eval(pred.strip(), ref.strip())
        total += dist
    return total / len(preds)

eed_score = extended_edit_distance(predictions, references)
print("Extended Edit Distance:", eed_score)

!pip install evaluate

!pip install rouge_score

import evaluate

# Load metrics
bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

# Prepare inputs for evaluate
results_bleu = bleu.compute(predictions=predictions, references=[[ref] for ref in references])
results_rouge = rouge.compute(predictions=predictions, references=references)

# Compute Exact Match
exact_matches = [pred.strip() == ref.strip() for pred, ref in zip(predictions, references)]
exact_match_score = sum(exact_matches) / len(exact_matches)

# Print results
print("📊 Evaluation Results:")
print(f"🔵 BLEU: {results_bleu['bleu']:.4f}")
print(f"🟢 ROUGE-L: {results_rouge['rougeL']:.4f}")
print(f"🟣 Exact Match: {exact_match_score:.4f}")

"""from google.colab import drive
drive.mount('/content/drive')

# Save everything together
torch.save({
    'model_state_dict': model.state_dict(),
    'clip_model_state_dict': clip_model.state_dict(),
    't5_model_state_dict': t5_model.state_dict(),  # includes LoRA
}, "/content/drive/MyDrive/tikzgen_model2.pt")
"""